{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c05e65-82da-4445-9577-f56809005b0b",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) using Foundation Models in SageMaker\n",
    "\n",
    "In this notebook we demonstrate how to use Retrieval Augmented Generation (RAG) to build a question-and-answer chatbot to converse with the **SEC Schedule 14A document** using Foundation Models in SageMaker.\n",
    "\n",
    "Foundation models are usually trained offline, making the model agnostic to any data that is created after the model was trained. Additionally, foundation models are trained on very general domain corpora, making them less effective for domain-specific tasks. Retrieval Augmented Generation (RAG) is used to retrieve data from outside a foundation model and augment your prompts by adding the relevant retrieved data in context. For more information about RAG model architectures, see [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401).\n",
    "\n",
    "With RAG, the external data used to augment your prompts can come from multiple data sources, such as a document repositories, databases, or APIs. The first step is to convert your documents and any user queries into a compatible format to perform relevancy search. To make the formats compatible, a document collection, or knowledge library, and user-submitted queries are converted to numerical representations using embedding language models. Embedding is the process by which text is given numerical representation in a vector space. RAG model architectures compare the embeddings of user queries within the vector of the knowledge library. The original user prompt is then appended with relevant context from similar documents within the knowledge library. This augmented prompt is then sent to the foundation model. You can update knowledge libraries and their relevant embeddings asynchronously.\n",
    "\n",
    "In the previous sections of this workshop, you deployed the **FLAN-T5** Foundation Model to SageMaker Endpoints and used these models for various Natural Language Processing (NLP) tasks such as text summarization, common sense reasoning, translation and question and answering. In this section, we will use this SageMaker endpoints to create vector embeddings that are stored in Amazon OpenSearch. We then use these embeddings in a RAG-model for a question-and-answer chatbot. The diagram below depicts this architecture.\n",
    "\n",
    "We will also use **LangChain**, an opensource framework for developing and interfacing with applications powered by language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3949f7e8-34a7-4f87-b359-853c48d17f0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Rag Architecture](../images/10-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b267d-6e22-48ec-8ce4-d3f9f6cf4fc3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The following are the prerequisites for this notebook:\n",
    "1. Deploy the SageMaker Jumpstart Model called `FLAN-T5-XL` ***OR*** Run the Jupyter Notebook titled `01-deploy-text2text-model.jpynb`. \n",
    "2. Deploy the SageMaker Jumpstart Model called `GPT-J 6B Embedding FP16` text embeddings model.\n",
    "3. [Not required if you do step 2.] Run the Jupyter Notebook titled `02-deploy-text2emb-model.jpynb`. This notebook deploys the gpt-j-6b-fp16 LLM to a SageMaker Endpoint.\n",
    "4. [Non-AWS Event] Run the Jupyter Notebook titled `03-create-vector-store.jpynb`. This notebook creates an Amazon OpenSearch Cluster and required Index for the vector database. This notebook is not required if you are running an AWS Event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d8c09-007d-4c00-86f6-6d26932d1d88",
   "metadata": {},
   "source": [
    "## Install Required Python Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88c459e-6031-4bda-a882-905f8491ee68",
   "metadata": {
    "tags": []
   },
   "source": [
    "_*IMPORTANT*_: Ensure you are running Pythin 3.9+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0f9b6-4de9-4896-b0e6-c94ec241a81c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Set Up Kernel and Required Dependencies\n",
    "\n",
    "First, check that the correct kernel is chosen.\n",
    "\n",
    "<img src=\"img/kernel_set_up_03.png\" width=\"300\"/>\n",
    "\n",
    "You can click on that to see and check the details of the image, kernel, and instance type.\n",
    "\n",
    "<img src=\"img/w3_kernel_and_instance_type_03.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7426d06-6974-49a0-98f8-e8135eabf2b2",
   "metadata": {},
   "source": [
    "# NOTE:  YOU CANNOT CONTINUE UNTIL THE KERNEL IS STARTED\n",
    "# ### PLEASE WAIT UNTIL THE KERNEL IS STARTED BEFORE CONTINUING!!! ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2cedb2-067a-40a1-99ed-7acccbc2700f",
   "metadata": {},
   "source": [
    "# Use `Shift+Enter` to Run Each Cell\n",
    "\n",
    "Use `Shift+Enter` on the cell below to see the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d0d4b-1a67-42d7-9c53-9c89ae6e0d34",
   "metadata": {},
   "source": [
    "# Click `Kernel` => `Restart Kernel and Run All Cells` to Run All Cells\n",
    "![](img/restart-kernel-and-run-all-cells.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4673292a-807d-43cf-a654-f9c4b6296e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.9.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Get the Python version.\n",
    "python_version = sys.version_info\n",
    "\n",
    "# Check if the Python version is above 3.9.\n",
    "if python_version.major < 3 or python_version.minor < 9:\n",
    "  # Raise an error message if the Python version is not above 3.9.\n",
    "  raise Exception(\"Python version must be above 3.9.\")\n",
    "\n",
    "# Print a success message if the Python version is above 3.9.\n",
    "print(\"Python version is above 3.9.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3476b4c-559b-4da1-b524-14ca011e04f9",
   "metadata": {},
   "source": [
    "Begin by installing the required python libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d992e1-9188-4977-a485-7092d0b9a6c7",
   "metadata": {},
   "source": [
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s below. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd04309a-1b91-4060-8ba2-9bfefe758294",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 1)) (5.4.1)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 2)) (0.0.310)\n",
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 3)) (0.0.1)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 4)) (4.9.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 5)) (4.64.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 6)) (2023.10.3)\n",
      "Requirement already satisfied: opensearch-py in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 7)) (2.3.1)\n",
      "Requirement already satisfied: boto3==1.26.73 in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 8)) (1.26.73)\n",
      "Requirement already satisfied: pandas==1.5.3 in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 9)) (1.5.3)\n",
      "Requirement already satisfied: Pillow==9.4.0 in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 10)) (9.4.0)\n",
      "Requirement already satisfied: streamlit==1.20.0 in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 11)) (1.20.0)\n",
      "Requirement already satisfied: requests-aws4auth in /opt/conda/lib/python3.9/site-packages (from -r ../requirements.txt (line 14)) (1.2.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from boto3==1.26.73->-r ../requirements.txt (line 8)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.9/site-packages (from boto3==1.26.73->-r ../requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.73 in /opt/conda/lib/python3.9/site-packages (from boto3==1.26.73->-r ../requirements.txt (line 8)) (1.29.165)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas==1.5.3->-r ../requirements.txt (line 9)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas==1.5.3->-r ../requirements.txt (line 9)) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.9/site-packages (from pandas==1.5.3->-r ../requirements.txt (line 9)) (1.23.5)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.8.1b0)\n",
      "Requirement already satisfied: semver in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (3.0.1)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.10.2)\n",
      "Requirement already satisfied: requests>=2.4 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (2.28.2)\n",
      "Requirement already satisfied: validators>=0.2 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.22.0)\n",
      "Requirement already satisfied: blinker>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (1.6.2)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (11.0.0)\n",
      "Requirement already satisfied: altair<5,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (4.2.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (4.4.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (5.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (12.6.0)\n",
      "Requirement already satisfied: tzlocal>=1.1 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (5.1)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (3.1.37)\n",
      "Requirement already satisfied: pympler>=0.9 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.12 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (3.20.2)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (8.1.2)\n",
      "Requirement already satisfied: packaging>=14.1 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (23.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (4.13.0)\n",
      "Requirement already satisfied: watchdog in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /opt/conda/lib/python3.9/site-packages (from streamlit==1.20.0->-r ../requirements.txt (line 11)) (6.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (8.2.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (2.0.21)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (0.6.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.40 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (0.0.43)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: anyio<4.0 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (1.10.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.9/site-packages (from langchain->-r ../requirements.txt (line 2)) (3.8.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.9/site-packages (from bs4->-r ../requirements.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: certifi>=2022.12.07 in /opt/conda/lib/python3.9/site-packages (from opensearch-py->-r ../requirements.txt (line 7)) (2022.12.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from opensearch-py->-r ../requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from opensearch-py->-r ../requirements.txt (line 7)) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 2)) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r ../requirements.txt (line 2)) (22.2.0)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.9/site-packages (from altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.9/site-packages (from altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (4.19.1)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.9/site-packages (from altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.9/site-packages (from anyio<4.0->langchain->-r ../requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.9/site-packages (from anyio<4.0->langchain->-r ../requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.9/site-packages (from anyio<4.0->langchain->-r ../requirements.txt (line 2)) (1.1.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ../requirements.txt (line 2)) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r ../requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.9/site-packages (from gitpython!=3.1.19->streamlit==1.20.0->-r ../requirements.txt (line 11)) (4.0.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=1.4->streamlit==1.20.0->-r ../requirements.txt (line 11)) (3.13.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain->-r ../requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.11.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (2.14.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich>=10.11.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.9.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r ../requirements.txt (line 2)) (2.0.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.9/site-packages (from beautifulsoup4->bs4->-r ../requirements.txt (line 3)) (2.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.20.0->-r ../requirements.txt (line 11)) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (2.1.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.30.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.9/site-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit==1.20.0->-r ../requirements.txt (line 11)) (0.10.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r ../requirements.txt (line 2)) (1.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb06ec0-d912-4a51-8b2e-8c898dd4041d",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c5c149-aa23-496e-b036-10ffc73e5615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup SageMaker Session\n",
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d3e92ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import langchain \n",
    "from langchain.document_loaders import UnstructuredHTMLLoader,BSHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.vectorstores import OpenSearchVectorSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5dcda-025f-41c3-b254-2f3e0465159f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Please Go Home > Deployments > Endpoints and copy the names of the Text-Embed and Text2Text models deployed in the Lab number 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa637d89-c6cf-4206-9041-409ae77c7fa7",
   "metadata": {},
   "source": [
    "![Endpoint](../images/50-navigate-to-endpoints.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d1099-a13f-4670-8279-b1a6431a46e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### They will be something like: \"jumpstart-dft-hf-textembedding-yyyy-mm-dd-hh-min-sec-msc\" and \"jumpstart-example-huggingface-text2text-yyyy-mm-dd-hh-min-sec-msc\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48383c71-2dd5-4014-b3e4-c305070326ff",
   "metadata": {},
   "source": [
    "<img src=\"../images/50-confirm-endpoints.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3cbf0e14-5253-466c-a7e2-deb5bc3cd085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Text2Text Model is:  jumpstart-example-huggingface-text2text-2023-10-06-19-56-29-081\n",
      "Your Text Embedding Model is:  jumpstart-example-huggingface-textembed-2023-10-06-20-10-28-812\n"
     ]
    }
   ],
   "source": [
    "# Replace the name of the endpoint here\n",
    "llm_endpoint_name =\"jumpstart-example-huggingface-text2text-2023-10-06-19-56-29-081\"\n",
    "print('Your Text2Text Model is: ' ,llm_endpoint_name)\n",
    "\n",
    "# Set embeddings model endpoint to jumpstart model endpoint\n",
    "# Replace the name of the endpoint here\n",
    "embeddings_model_endpoint_name = \"jumpstart-example-huggingface-textembed-2023-10-06-20-10-28-812\"\n",
    "print('Your Text Embedding Model is: ' ,embeddings_model_endpoint_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af133b35-95fe-4ae1-bf09-7341188764c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search-genaiopensearch-ugzhlx2nc4yikjjyskp7cux7wy.us-east-1.es.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "# Set variables for Amazon OpenSearch\n",
    "\n",
    "import boto3\n",
    "cfn_client = boto3.client('cloudformation')\n",
    "stack_name = \"genai-rag-workshop-studio\"\n",
    "\n",
    "response = cfn_client.describe_stacks(\n",
    "    StackName = stack_name,\n",
    ")\n",
    "\n",
    "outputs = response['Stacks'][0]['Outputs'] \n",
    "\n",
    "opensearch_host_id= next(output['OutputValue'] for output in outputs\n",
    "        if output['OutputKey'] == 'Opensearchhostid')\n",
    "\n",
    "#Please confirm the name of the OpenSearch Index\n",
    "_aos_host = opensearch_host_id\n",
    "_aos_index = 'fsi-demo-knn'\n",
    "\n",
    "print(_aos_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166d759",
   "metadata": {},
   "source": [
    "## Chunk your Data and Load into Amazon OpenSearch\n",
    "\n",
    "In this section we will chunk the data into smaller documents. Chunking is a technique for splitting large texts into smaller chunks. It is an important step as it optimizes the relevance of the search query for our RAG-model. Which in turn improves the quality of the chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4a2d6bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = BSHTMLLoader(\"../data/14A/0000003153-20-000004.html\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4fd7c9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 document(s) in your data\n",
      "There are 153880 characters in your document\n"
     ]
    }
   ],
   "source": [
    "print (f'You have {len(data)} document(s) in your data')\n",
    "print (f'There are {len(data[0].page_content)} characters in your document')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af9b604",
   "metadata": {},
   "source": [
    "\n",
    "### Then we select  chunk size and overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb3c6f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now you have 110 documents\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1600, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'Now you have {len(docs)} documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b5954f-fc77-45c7-9cb9-b7fc2fe24390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to process document\n",
    "\n",
    "import regex as re\n",
    "\n",
    "def postproc(s):\n",
    "    s = s.replace(u'\\xa0', u' ') # no-break space \n",
    "    s = s.replace('\\n', ' ') # new-line\n",
    "    s = re.sub(r'\\s+', ' ', s) # multiple spaces\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "725f1429-c9c3-49b1-a779-eda1c901232e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.page_content = postproc(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a10df400-95c0-41ba-a5cf-097af738d30a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='UNITED STATESSECURITIES AND EXCHANGE COMMISSIONWASHINGTON, D.C. 20549SCHEDULE 14A INFORMATIONProxy Statement Pursuant To Section 14(a)of the Securities Exchange Act of 1934xFiled by the RegistrantoFiled by a party other than the RegistrantCheck the appropriate box:oPreliminary proxy statementoConfidential, for use of the Commission only (as permitted by Rule 14a-6(e)(2))xDefinitive proxy statementoDefinitive additional materialsoSoliciting material under Rule 14a-12ALABAMA POWER COMPANY(Name of Registrant as Specified in Its Charter)(Name of Person(s) Filing Proxy Statement, if Other Than the Registrant)Payment of Filing Fee (Check the appropriate box):xNo fee required. oFee computed on table below per Exchange Act Rules 14a-6(i)(1) and 0-11. (1)Title of each class of securities to which transaction applies: (2)Aggregate number of securities to which transaction applies: (3)Per unit price or other underlying value of transaction computed pursuant to Exchange Act Rule 0-11 (set forth the amount on which the filing fee is calculated and state how it was determined): (4)Proposed maximum aggregate value of transaction: (5)Total fee paid: oFee paid previously with preliminary materials.oCheck box if any part of the fee is offset as provided by Exchange Act Rule 0-11(a)(2) and identify the filing for which the offsetting fee was paid previously. Identify the previous filing by registration statement number, or the Form or Schedule and the date of its filing. (1)Amount Previously Paid: (2)Form, Schedule or Registration Statement No.: (3)Filing', metadata={'source': '../data/14A/0000003153-20-000004.html', 'title': 'None'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review the first document for correctness\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0c0820-0080-4676-94a3-38594470ebd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limit the number of total chunks to 1000\n",
    "MAX_DOCS = 1000\n",
    "if len(docs) > MAX_DOCS:\n",
    "    docs = docs[:MAX_DOCS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9342b6-db4d-4509-b0c4-10c8ca08b809",
   "metadata": {},
   "source": [
    "### Prior to populating a vector store, compute embedding to validate the smoothness / no exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a4079-5d48-4fa3-8bcc-9ba7a937102b",
   "metadata": {},
   "source": [
    "We begin by extending the LangChain SageMakerEndpointEmbeddings Class to create a custom embeddings function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "635f15ca-e6ec-4f4c-af9f-0a455cba1f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# extend the SagemakerEndpointEmbeddings class from langchain to provide a custom embedding function\n",
    "class SagemakerEndpointEmbeddingsJumpStart(SagemakerEndpointEmbeddings):\n",
    "    def embed_documents(\n",
    "        self, texts: List[str], chunk_size: int = 5\n",
    "    ) -> List[List[float]]:\n",
    "        \"\"\"Compute doc embeddings using a SageMaker Inference Endpoint.\n",
    "\n",
    "        Args:\n",
    "            texts: The list of texts to embed.\n",
    "            chunk_size: The chunk size defines how many input texts will\n",
    "                be grouped together as request. If None, will use the\n",
    "                chunk size specified by the class.\n",
    "\n",
    "        Returns:\n",
    "            List of embeddings, one for each text.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        _chunk_size = len(texts) if chunk_size > len(texts) else chunk_size\n",
    "        st = time.time()\n",
    "        for i in range(0, len(texts), _chunk_size):\n",
    "            response = self._embedding_func(texts[i:i + _chunk_size])\n",
    "            results.extend(response)\n",
    "        time_taken = time.time() - st\n",
    "        logger.info(f\"got results for {len(texts)} in {time_taken}s, length of embeddings list is {len(results)}\")\n",
    "        return results\n",
    "\n",
    "\n",
    "# class for serializing/deserializing requests/responses to/from the embeddings model\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs={}) -> bytes:\n",
    "\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8') \n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        embeddings = response_json[\"embedding\"]\n",
    "        if len(embeddings) == 1:\n",
    "            return [embeddings[0]]\n",
    "        return embeddings\n",
    "    \n",
    "\n",
    "def create_sagemaker_embeddings_from_js_model(embeddings_model_endpoint_name: str, aws_region: str) -> SagemakerEndpointEmbeddingsJumpStart:\n",
    "    # all set to create the objects for the ContentHandler and \n",
    "    # SagemakerEndpointEmbeddingsJumpStart classes\n",
    "    content_handler = ContentHandler()\n",
    "\n",
    "    # note the name of the LLM Sagemaker endpoint, this is the model that we would\n",
    "    # be using for generating the embeddings\n",
    "    embeddings = SagemakerEndpointEmbeddingsJumpStart( \n",
    "        endpoint_name=embeddings_model_endpoint_name,\n",
    "        region_name=aws_region, \n",
    "        content_handler=content_handler\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04666cc-99c4-4204-a316-9a24dec99333",
   "metadata": {},
   "source": [
    "Next, we create the embeddings object and batch the create the document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed872e9-387d-4d60-8567-4866ed365677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = create_sagemaker_embeddings_from_js_model(embeddings_model_endpoint_name, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838b2843",
   "metadata": {},
   "source": [
    "### Create embeddings of your documents to get ready for semantic search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c8e830f-bf12-41fd-b500-e3a7ee30661d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from opensearchpy import RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "\n",
    "service = \"es\"\n",
    "credentials = boto3.Session().get_credentials()\n",
    "\n",
    "awsauth = AWS4Auth(\n",
    "    credentials.access_key,\n",
    "    credentials.secret_key,\n",
    "    aws_region,\n",
    "    service,\n",
    "    session_token=credentials.token,\n",
    ")\n",
    "\n",
    "docsearch = OpenSearchVectorSearch.from_texts(\n",
    "        texts = [d.page_content for d in docs],\n",
    "        embedding = embeddings,\n",
    "        metadatas = [d.metadata for d in docs],\n",
    "        opensearch_url = [{'host': _aos_host, 'port': 443}],\n",
    "        index_name = _aos_index,\n",
    "        http_auth = awsauth,\n",
    "        use_ssl = True,\n",
    "        pre_delete_index=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        timeout=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35dcd9",
   "metadata": {},
   "source": [
    "## Question answering over Documents \n",
    "\n",
    "So far, we have chunked a large document into smaller ones, created vector embedding and stored them in an OpenSearch Vector Database. Now, we can answer questions over this document data.\n",
    "\n",
    "Since we have created an index over the data, we can do a semantic search over the documents; this way only the most relevant documents to answer the question are passed via the prompt to the Large Language Model (LLM). You save both time and money by not passing all the documents to the LLM.\n",
    "\n",
    "We use langchains **question_answering** `stuff` document chain in this example. Further details on Document Chains can be found by visiting the langchain [documentation, here](https://python.langchain.com/docs/modules/chains/document/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c15dbb66-0c50-4839-b616-54b1b20e124f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import json\n",
    "\n",
    "class SageMakerLLMContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        # input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        input_str = json.dumps({\"text_inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        # return response_json[0][\"generated_text\"]\n",
    "        return response_json['generated_texts'][0]\n",
    "\n",
    "\n",
    "sagemaker_llm_content_handler= SageMakerLLMContentHandler()\n",
    "\n",
    "chain = load_qa_chain(\n",
    "    llm=SagemakerEndpoint(\n",
    "        endpoint_name=llm_endpoint_name,\n",
    "        # credentials_profile_name=\"credentials-profile-name\",\n",
    "        region_name=aws_region,\n",
    "        model_kwargs={\"temperature\": 1e-10},\n",
    "        content_handler=sagemaker_llm_content_handler,\n",
    "    ),\n",
    "    chain_type=\"stuff\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f560cce3-5891-41ef-b6cb-59efbafe2056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phillip M. Webb'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are the directors?\"\n",
    "sim_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain.run(input_documents=sim_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f67ea7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phillip M. Webb'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who are the nominees?\"\n",
    "sim_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "chain.run(input_documents=sim_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "485bc94c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How old is Mark A. Crosswhite?\n",
      "A: unanswerable\n",
      "\n",
      "---\n",
      "\n",
      "Q: What is Mark A. Crosswhite current position and what is the name of the organization he/she currently works for?\n",
      "A: not enough information\n",
      "\n",
      "---\n",
      "\n",
      "Q: How old is Phillip M. Webb?\n",
      "A: 62\n",
      "\n",
      "---\n",
      "\n",
      "Q: What is Phillip M. Webb current position and what is the name of the organization he/she currently works for?\n",
      "A: President of Webb Concrete and Building Materials\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for person in ['Mark A. Crosswhite', 'Phillip M. Webb']:\n",
    "    for query_template in [\n",
    "                    \"How old is {PERSON}?\",\n",
    "                    \"What is {PERSON} current position and what is the name of the organization he/she currently works for?\"\n",
    "                 ]:\n",
    "    \n",
    "        query = query_template.format(PERSON=person)\n",
    "        print('Q:', query)\n",
    "\n",
    "        sim_docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "        answer = chain.run(input_documents=sim_docs, question=query)    \n",
    "        print('A:', answer)\n",
    "        print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165f5ff-d6d5-414e-a4de-b465798a42a8",
   "metadata": {},
   "source": [
    "#### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c50d6-e2c7-4288-966d-e29f8c04bbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
